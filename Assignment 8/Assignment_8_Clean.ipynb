{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "c1130c0e",
      "metadata": {
        "id": "c1130c0e"
      },
      "source": [
        "# Follow these instructions:\n",
        "\n",
        "Once you are finished, ensure to complete the following steps.\n",
        "\n",
        "1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
        "\n",
        "2.  Fix any errors which result from this.\n",
        "\n",
        "3.  Repeat steps 1. and 2. until your notebook runs without errors.\n",
        "\n",
        "4.  Submit your completed notebook to OWL by the deadline."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "julian-uzbekistan",
      "metadata": {
        "id": "julian-uzbekistan"
      },
      "source": [
        "## Assignment Week 8: Text Mining using Dimensionality Reduction Methods [_/100 Marks]\n",
        "\n",
        "This dataset comes from the #TidyTuesday repository and represents 2122 TV shows. In this assignment, we will apply dimensionality reduction methods to improve our understanding of text data and to predict the number of seasons of the TV shows. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "respective-contrast",
      "metadata": {
        "id": "respective-contrast",
        "outputId": "9792dbea-af52-4587-f95b-018dc173856a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\yasin\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# !pip install umap-learn\n",
        "import umap\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import PCA, TruncatedSVD\n",
        "import sklearn.feature_extraction.text as sktext\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, auc\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from itertools import product\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "seed = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "declared-federal",
      "metadata": {
        "id": "declared-federal"
      },
      "source": [
        "## Task 1: Decomposition of the texts [ /66 marks]\n",
        "\n",
        "### Question 1.1\n",
        "\n",
        "The dataset comes with the a few text variables and a categorical variable which represents whether a TV show has one season or more than one season. Import the data and create a new variable called 'full_description' by combining the three columns title, listed_in, and description. Keep the two columns,'duration' and 'full_description', and remove the rest. Do binary encoding for the traget variable 'duration'. Assign 1 to 'More than one season' and 0 to 'One season'. In the \"full_description\" column replace the word \"Sci-Fi\" with the word \"Sci_Fi\" since we would want to treat \"Sci-Fi\" as single word. Select the full_description column and display its first 10 rows. Use sklearn's `TfidfVectorizer` to eliminate accents, special characters, and stopwords (please see below to find out what stopwords need to be removed). In addition, make sure to eliminate words that appear in less than 5% of documents and those that appear in over 95%. You can also set `sublinear_tf` to `True`. After that, split the data into train and test with `test_size = 0.2` and `seed = seed`. Calculate the [Tf-Idf transform](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html) for both train and test. Note that you need to fit and transform the inputs for the train set but you only need to transform the inputs for the test set. Don't forget to turn the sparse matrices to dense ones after you apply the `Tf-Idf` transform.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f703a98",
      "metadata": {
        "id": "1f703a98"
      },
      "outputs": [],
      "source": [
        "# Load the data [ /1 mark]\n",
        "\n",
        "# Create a new variable called 'full_description' by combining the three columns title, listed_in, and description [ /2 marks]\n",
        "# use an empty space to concat these three columns\n",
        "\n",
        "# Keep two columns,'duration' and 'full_description', and remove the rest\n",
        "\n",
        "# Do binary encoding for the traget variable 'duration'. Assign 1 to 'More than one season' and 0 to 'One season' [ \\1 mark]\n",
        "\n",
        "# In the \"full_description\" column replace the word \"Sci-Fi\" with the word \"Sci_Fi\" [ \\1 mark]\n",
        "\n",
        "# Select the following column and display its first 10 rows: full_description [ /1 mark]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "clear-imperial",
      "metadata": {
        "id": "clear-imperial"
      },
      "outputs": [],
      "source": [
        "# Defining the TfIDFTransformer [ /4 marks]\n",
        "# Define a vectors of stop words: stop words list must contain 'english' stop words, 'shows', and 'tv' \n",
        "\n",
        "\n",
        "# Train/test split [ /2 marks]\n",
        "\n",
        "# Calculate the Tf-Idf transform [ /2 marks]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bacterial-calvin",
      "metadata": {
        "id": "bacterial-calvin"
      },
      "source": [
        "From here on, you will use the variables `TfIDF_train` and `TfIDF_test` as the input for the different tasks, and the `y_train` and `y_test` labels for each dataset (if required). Print the number of indices in the ouput using [`TfIDFTransformer.get_feature_names()` method](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "272f10cb",
      "metadata": {
        "id": "272f10cb"
      },
      "outputs": [],
      "source": [
        "# Print the number of indices [ /2 marks]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tribal-scholarship",
      "metadata": {
        "id": "tribal-scholarship"
      },
      "source": [
        "### Question 1.2\n",
        "Now we have the TfIDF matrix so we can start working on the data. We hope to explore what some commonly occuring concepts are in the 'full_description' column. We can do this using PCA. A PCA transform of the TF-IDF matrix will give us a basis of the text data, each component representing a *concept* or set of words that are correlated. Correlation in text can be interpreted as a relation to a similar topic. Calculate a PCA transform of the training data using the **maximum** number of concepts possible. Make a plot of the explained variance that shows the cumulative explained variance per number of concepts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "valid-uganda",
      "metadata": {
        "id": "valid-uganda"
      },
      "outputs": [],
      "source": [
        "# Apply PCA on training data and get the explained variance [ / 4 marks]\n",
        "\n",
        "# Plotting explained variance with number of concepts [ / 4 marks]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeeaebd3",
      "metadata": {
        "id": "eeeaebd3"
      },
      "source": [
        "**Question:** Exactly how many concepts do we need to correctly explain at least 80% of the data?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fitted-rogers",
      "metadata": {
        "id": "fitted-rogers"
      },
      "outputs": [],
      "source": [
        "# To get the exact index where the variance is above 80% [ / 4 marks]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "224b4fd3",
      "metadata": {
        "id": "224b4fd3"
      },
      "source": [
        "**Your Answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "electronic-calgary",
      "metadata": {
        "id": "electronic-calgary"
      },
      "source": [
        "### Question 1.3\n",
        "\n",
        "Let's examine the first three concepts by looking how many variance they explained and showing the 10 words that are the most important in each of these three concepts (as revealed by the absolute value of the PCA weight in each concept).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lesser-prison",
      "metadata": {
        "id": "lesser-prison"
      },
      "outputs": [],
      "source": [
        "# Explained variance [ / 2 marks]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "based-deviation",
      "metadata": {
        "id": "based-deviation"
      },
      "outputs": [],
      "source": [
        "# Get 10 most important words for each component [ / 4 marks]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gorgeous-montana",
      "metadata": {
        "id": "gorgeous-montana"
      },
      "outputs": [],
      "source": [
        "# Words for concept 1 [ / 2 marks]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mature-cyprus",
      "metadata": {
        "id": "mature-cyprus"
      },
      "outputs": [],
      "source": [
        "# Words for concept 2 [ / 2 marks]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "proprietary-contributor",
      "metadata": {
        "id": "proprietary-contributor"
      },
      "outputs": [],
      "source": [
        "# Words for concept 3 [ / 2 marks]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "outstanding-hepatitis",
      "metadata": {
        "id": "outstanding-hepatitis"
      },
      "source": [
        "### Question 1.4\n",
        "\n",
        " Apply the PCA transformation to the test dataset. Use only the first two components and make a scatter plot of the TV shows. Identify the 'More than one season' TV shows, and 'One season' TV shows by colouring points with different colours. Make sure to add a legend to your plot!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ruled-central",
      "metadata": {
        "id": "ruled-central"
      },
      "outputs": [],
      "source": [
        "# Apply PCA to the test dataset [ / 2 marks]\n",
        "\n",
        "\n",
        "# Plot the two different set of points with different markers and labels [ /4 marks]\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "present-sunglasses",
      "metadata": {
        "id": "present-sunglasses"
      },
      "source": [
        "**Question:** What can we say about where 'More than one season' TV shows and 'One season' TV shows lie in our plot? Could we use these concepts to discriminate these cases? If yes, why? If no, why not? Discuss your findings. [ /2 marks]\n",
        " \n",
        "**Your answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dt7hcc3noPEU",
      "metadata": {
        "id": "dt7hcc3noPEU"
      },
      "source": [
        "### Question  1.5\n",
        "\n",
        "Repeat the process above, only now using a UMAP projection with two components. Test all combinations of ```n_neighbors=[2, 10, 25, 35, 45]``` and ```min_dist=[0.1, 0.25, 0.5, 1]``` over the train data and choose the projection that you think is best, and apply it over the test data. Use 1000 epochs, a cosine metric and random initialization. If you have more than 8GB of RAM (as in Colab), you may want to set ```low_memory=False``` to speed up computations.\n",
        "\n",
        "*Hint: [This link](https://stackoverflow.com/questions/16384109/iterate-over-all-combinations-of-values-in-multiple-lists-in-python) may be helpful.*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "k5s0D9iuoFBT",
      "metadata": {
        "id": "k5s0D9iuoFBT"
      },
      "outputs": [],
      "source": [
        "# Set parameters\n",
        "\n",
        "# Create UMAP and plots [ / 8 marks]\n",
        "\n",
        "\n",
        "    # Create plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KyMYOAMf4l_E",
      "metadata": {
        "id": "KyMYOAMf4l_E"
      },
      "source": [
        "**Question:** Which paramter would you choose? [ / 2 marks]\n",
        "\n",
        "**Your Answer:** "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-21cSLBty9gh",
      "metadata": {
        "id": "-21cSLBty9gh"
      },
      "outputs": [],
      "source": [
        "# Choose the paramters that you think are best and apply to test set [ / 4 marks]\n",
        "\n",
        "\n",
        "# Create plot [ /2 marks]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A40NKn9Y5tM9",
      "metadata": {
        "id": "A40NKn9Y5tM9"
      },
      "source": [
        "\n",
        "**Question:** How does the plot compare to the PCA one? [ /2 marks]\n",
        "\n",
        "**Your answer:** "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nuclear-landing",
      "metadata": {
        "id": "nuclear-landing"
      },
      "source": [
        "## Task 2: Benchmarking predictive capabilities of the compressed data [ / 34 marks]\n",
        "\n",
        "For this task, we will benchmark the predictive capabilities of the compressed data against the original one. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4137a1ce",
      "metadata": {
        "id": "4137a1ce"
      },
      "source": [
        "\n",
        "### Question 2.1 \n",
        "Train a regularized logistic regression over the original TfIDF train set using l2 regularization. Calculate the AUC score and plot the ROC curve for the original test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "frank-madison",
      "metadata": {
        "id": "frank-madison"
      },
      "outputs": [],
      "source": [
        "# Train and test using model LogisticRegressionCV [ /3 marks]\n",
        "\n",
        "# Define the model\n",
        "\n",
        "\n",
        "# Fit on the training dataset\n",
        "\n",
        "\n",
        "# Apply to the test dataset\n",
        "\n",
        "\n",
        "# Plot ROC curve and compute AUC score [ /2 marks]\n",
        "# Calculate the ROC curve points\n",
        "\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "\n",
        "# Create and show the plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d2f05ad",
      "metadata": {
        "id": "8d2f05ad"
      },
      "source": [
        "### Question 2.2 \n",
        "Train a regularized logistic regression over an SVD-reduced dataset (with 13 components) using l2 regularization. Calculate the AUC score and plot the ROC curve for the SVD-transformed test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "descending-picking",
      "metadata": {
        "id": "descending-picking"
      },
      "outputs": [],
      "source": [
        "# Apply SVD first [ / 4 marks]\n",
        "\n",
        "#Train and test using model LogisticRegressionCV [ /3 marks]\n",
        "\n",
        "\n",
        "# Plot ROC curve and compute AUC score [ /2 marks]\n",
        "# Calculate the ROC curve points\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "\n",
        "# Create and show the plot\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "814b2fc4",
      "metadata": {
        "id": "814b2fc4"
      },
      "source": [
        "### Question 2.3 \n",
        "Train a regularized logistic regression over the UMAP-reduced dataset (with 13 components using the same parameters as Task 1.5) using l2 regularization. Calculate the AUC score and plot the ROC curve for the UMAP-transformed test set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unnecessary-geology",
      "metadata": {
        "id": "unnecessary-geology"
      },
      "outputs": [],
      "source": [
        "# Apply UMAP first [ / 3 marks]\n",
        "\n",
        "\n",
        "#Train and test using model LogisticRegressionCV [ /4 marks]\n",
        "\n",
        "\n",
        "# Plot ROC curve and compute AUC score [ /2 marks]\n",
        "# Calculate the ROC curve points\n",
        "\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "\n",
        "# Create and show the plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1aaeef53",
      "metadata": {
        "id": "1aaeef53"
      },
      "source": [
        "### Question 2.4\n",
        "Train a XGBoost model over the SVD-reduced dataset prepared in Question 2.2. Calculate the AUC score and plot the ROC curve for the SVD-transformed test set. In your model set ``num_boost_round=10`` and ``early_stopping_rounds=2``. You need to do CV using the training dataset, and then get best iteration based on cross-validation results. Finally, train the model on full training dataset with best number of iterations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f6d05ed",
      "metadata": {
        "id": "7f6d05ed"
      },
      "outputs": [],
      "source": [
        "# Define XGBoost parameters\n",
        "params = {\n",
        "    \"objective\": \"binary:logistic\",\n",
        "    \"eval_metric\": \"logloss\",\n",
        "    \"max_depth\": 3,\n",
        "}\n",
        "\n",
        "# Define cross-validation object\n",
        "\n",
        "# Perform cross-validation with XGBoost [ \\3 marks]\n",
        "\n",
        "\n",
        "# Get best iteration based on cross-validation results [\\ 1 mark]\n",
        "\n",
        "# Train final model on full dataset with best number of iterations [\\ 2 mark]\n",
        "\n",
        "\n",
        "# Compute predicted probabilities on the test set [\\ 1 mark]\n",
        "\n",
        "# Plot ROC curve and compute AUC score [ /2 marks]\n",
        "# Calculate the ROC curve points\n",
        "\n",
        "\n",
        "# Save the AUC in a variable to display it. Round it first\n",
        "\n",
        "# Create and show the plot\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "damaged-peace",
      "metadata": {
        "id": "damaged-peace"
      },
      "source": [
        "### Question 2.5\n",
        "Compare the performance of the four models. Which one is the best. [ / 2 marks] \n",
        "\n",
        "**Your Answer:** "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "vscode": {
      "interpreter": {
        "hash": "3420a8792bbc8a921cecec9f5e200567f9d5b83365a03086ee32a665b051d9eb"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
